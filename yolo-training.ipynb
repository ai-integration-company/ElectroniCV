{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8966743,"datasetId":5344454,"databundleVersionId":9130673},{"sourceType":"datasetVersion","sourceId":8945531,"datasetId":5383022,"databundleVersionId":9108537},{"sourceType":"datasetVersion","sourceId":8939170,"datasetId":5378575,"databundleVersionId":9101838}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"id":"OEB6NfJva3kE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport shutil\nimport xml.etree.ElementTree as ET\nfrom matplotlib.patches import Rectangle\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"id":"R1GvIVCoI4vO","execution":{"iopub.status.busy":"2024-07-16T11:12:18.296113Z","iopub.execute_input":"2024-07-16T11:12:18.296411Z","iopub.status.idle":"2024-07-16T11:12:24.948003Z","shell.execute_reply.started":"2024-07-16T11:12:18.296385Z","shell.execute_reply":"2024-07-16T11:12:24.947099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport json\nfrom PIL import Image\n\ndef convert_xml_to_coco_and_segment(xml_file, output_json, images_folder, output_folder):\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    coco = {\n        \"info\": {},\n        \"licenses\": [],\n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": []\n    }\n\n    categories = {}\n    category_id = 1\n    for label in root.findall('.//label'):\n        category_name = label.find('name').text\n        if category_name.lower() == 'текст':\n            continue\n        categories[category_name] = category_id\n        coco['categories'].append({\n            \"id\": category_id,\n            \"name\": category_name,\n            \"supercategory\": \"none\"\n        })\n        category_id += 1\n\n    image_id = 1\n    annotation_id = 1\n    min_segment_size = 420\n    max_segment_size = 640\n\n    for image in tqdm(root.findall('.//image')):\n        file_name = image.get('name')\n        image_path = os.path.join(images_folder, file_name)\n        if not os.path.exists(image_path):\n            print(f\"Image file {image_path} does not exist.\")\n            continue\n        img = Image.open(image_path)\n        original_width, original_height = img.size\n\n        # Calculate non-intersecting segments\n        boxes = []\n        for box in image.findall('.//box'):\n            label = box.get('label')\n            if label not in categories:\n                continue\n            xmin = float(box.get('xtl'))\n            ymin = float(box.get('ytl'))\n            xmax = float(box.get('xbr'))\n            ymax = float(box.get('ybr'))\n            boxes.append((xmin, ymin, xmax, ymax, categories[label]))\n\n        # Create segments for each box\n        for box in boxes:\n            box_xmin, box_ymin, box_xmax, box_ymax, category_id = box\n\n            # Calculate the segment coordinates\n            start_x = max(0, box_xmax - max_segment_size)\n            end_x = min(box_xmin, original_width - min_segment_size)\n            start_y = max(0, box_ymax - max_segment_size)\n            end_y = min(box_ymin, original_height - min_segment_size)\n\n            end_x = min(start_x + max_segment_size, original_width)\n            end_y = min(start_y + max_segment_size, original_height)\n\n            if end_x - start_x < min_segment_size:\n                start_x = max(0, end_x - min_segment_size)\n            if end_y - start_y < min_segment_size:\n                start_y = max(0, end_y - min_segment_size)\n\n            segment = img.crop((start_x, start_y, end_x, end_y))\n            segment_file_name = f\"segment_{image_id}_{start_x}_{start_y}.png\"\n            segment_path = os.path.join(output_folder, segment_file_name)\n            segment.save(segment_path)\n\n            # Update COCO image data\n            coco['images'].append({\n                \"id\": image_id,\n                \"file_name\": segment_file_name,\n                \"height\": end_y - start_y,\n                \"width\": end_x - start_x\n            })\n\n            # Update annotations for this segment\n            for box in boxes:\n                if start_x < box[2] and end_x > box[0] and start_y < box[3] and end_y > box[1]:\n                    xmin = max(box[0] - start_x, 0)\n                    ymin = max(box[1] - start_y, 0)\n                    xmax = min(box[2] - start_x, end_x - start_x)\n                    ymax = min(box[3] - start_y, end_y - start_y)\n                    width = xmax - xmin\n                    height = ymax - ymin\n\n\n                    coco['annotations'].append({\n                        \"id\": annotation_id,\n                        \"image_id\": image_id,\n                        \"category_id\": box[4],\n                        \"bbox\": [xmin, ymin, width, height],\n                        \"area\": width * height,\n                        \"segmentation\": [],\n                        \"iscrowd\": 0\n                    })\n                    annotation_id += 1\n\n            image_id += 1\n\n    # Save the updated COCO data\n    with open(output_json, 'w') as json_file:\n        json.dump(coco, json_file, indent=4)\n\n    print(f\"Conversion completed. {len(coco['images'])} images and {len(coco['annotations'])} annotations created.\")\n\n# Example usage:\n# convert_xml_to_coco_and_segment('path_to_xml_file.xml', 'output.json', 'images_folder', 'output_folder')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T11:12:40.131538Z","iopub.execute_input":"2024-07-16T11:12:40.131892Z","iopub.status.idle":"2024-07-16T11:12:40.153146Z","shell.execute_reply.started":"2024-07-16T11:12:40.131862Z","shell.execute_reply":"2024-07-16T11:12:40.152236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in [330, 482, 331, 333, 334]:\n    cvat_json_path = f'/kaggle/input/electrocv/EKF AI Challenge/{i}/annotations.xml'\n    output_coco_json_path = f'/kaggle/working/coco_{i}.json'\n    images_folder = f'/kaggle/input/electrocv/EKF AI Challenge/{i}/images'\n    convert_xml_to_coco_and_segment(cvat_json_path, output_coco_json_path, images_folder, f'/kaggle/working/{i}/')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T11:12:41.378260Z","iopub.execute_input":"2024-07-16T11:12:41.379167Z","iopub.status.idle":"2024-07-16T11:15:44.055005Z","shell.execute_reply.started":"2024-07-16T11:12:41.379129Z","shell.execute_reply":"2024-07-16T11:15:44.054111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nfrom shutil import copy2\n\ndef merge_datasets(image_folders, json_files, output_image_folder, output_json_file):\n    if not os.path.exists(output_image_folder):\n        os.makedirs(output_image_folder)\n\n    merged_coco = {\n        \"info\": {},\n        \"licenses\": [],\n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": []\n    }\n\n    # Обработка аннотаций и копирование изображений\n    image_id = 1\n    annotation_id = 1\n    category_ids = {}\n    category_id = 1\n    category_id_mapping = {}  # Новый словарь для сопоставления старых и новых ID категорий\n    \n    for json_file in json_files:\n        with open(json_file, 'r') as file:\n            coco_data = json.load(file)\n            \n            # Обработка категорий\n            if 'categories' in coco_data:\n                for category in coco_data['categories']:\n                    if category['name'] not in category_ids:\n                        category_ids[category['name']] = category_id\n                        category_id_mapping[category['id']] = category_id\n                        new_category = category.copy()\n                        new_category['id'] = category_id\n                        merged_coco['categories'].append(new_category)\n                        category_id += 1\n                    else:\n                        category_id_mapping[category['id']] = category_ids[category['name']]\n            \n            # Обработка изображений и аннотаций\n            if 'images' in coco_data:\n                for image in coco_data['images']:\n                    new_image = image.copy()\n                    new_image['id'] = image_id\n                    # Копирование изображений в новую папку\n                    source_path = os.path.join(image_folders[json_files.index(json_file)], image['file_name'])\n                    destination_file_name = f\"{image_id}_{os.path.basename(image['file_name'])}\"  # Новый уникальный имя файла\n                    destination_path = os.path.join(output_image_folder, destination_file_name)\n                    copy2(source_path, destination_path)\n                    new_image['file_name'] = destination_file_name\n                    merged_coco['images'].append(new_image)\n                    \n                    # Обновление аннотаций\n                    for annotation in coco_data['annotations']:\n                        if annotation['image_id'] == image['id']:\n                            new_annotation = annotation.copy()\n                            new_annotation['id'] = annotation_id\n                            new_annotation['image_id'] = image_id\n                            new_annotation['category_id'] = category_id_mapping[annotation['category_id']]\n                            merged_coco['annotations'].append(new_annotation)\n                            annotation_id += 1\n                    \n                    image_id += 1\n\n    # Сохранение объединённого файла JSON\n    with open(output_json_file, 'w') as file:\n        json.dump(merged_coco, file, indent=4)\n\n# Пути к папкам с изображениями и файлами JSON\nimage_folders = [\"/kaggle/working/331\", \"/kaggle/working/330\", \"/kaggle/working/334\", \"/kaggle/working/482\", \"/kaggle/working/333\"]\njson_files = [\"/kaggle/working/coco_331.json\", \"/kaggle/working/coco_330.json\", \"/kaggle/working/coco_334.json\", \"/kaggle/working/coco_482.json\", \"/kaggle/working/coco_333.json\"]\n\n# Вызов функции для объединения\nmerge_datasets(image_folders, json_files, \"merged_dataset1\", \"merged_annotations1.json\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T11:15:44.056737Z","iopub.execute_input":"2024-07-16T11:15:44.057033Z","iopub.status.idle":"2024-07-16T11:15:54.120409Z","shell.execute_reply.started":"2024-07-16T11:15:44.057006Z","shell.execute_reply":"2024-07-16T11:15:54.119642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"m2XMvwzRtlge","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Define the augmentation pipeline\ndef get_rotation_transforms(angle):\n    return A.Compose([\n        A.Rotate(limit=(angle, angle), p=1.0),  # Rotate by a specific angle with 100% probability\n        A.RandomScale(scale_limit=0.1, p=0.1),  # Scale with low probability\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.1),  # Add noise with low probability\n        ToTensorV2()\n    ], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n\n# Load COCO annotations\ndef load_coco_annotations(coco_json_path):\n    with open(coco_json_path, 'r') as f:\n        coco_data = json.load(f)\n    return coco_data\n\n# Save augmented COCO annotations\ndef save_coco_annotations(coco_data, output_json_path):\n    with open(output_json_path, 'w') as f:\n        json.dump(coco_data, f, indent=4)\n\n# Augment images and annotations\ndef augment_images_and_annotations(coco_data, images_folder, output_images_folder):\n    rotation_angles = [90, 180, 270]\n    augmented_images = []\n    augmented_annotations = []\n    image_id = 1\n    annotation_id = 1\n\n    for image_info in tqdm(coco_data['images']):\n        image_path = os.path.join(images_folder, image_info['file_name'])\n        image = cv2.imread(image_path)\n        if image is None:\n            continue\n\n        annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_info['id']]\n        bboxes = [ann['bbox'] for ann in annotations]\n        category_ids = [ann['category_id'] for ann in annotations]\n\n        # Save original image and annotations\n        original_image_filename = f'original_image_{image_id}.jpg'\n        original_image_path = os.path.join(output_images_folder, original_image_filename)\n        cv2.imwrite(original_image_path, image)\n\n        augmented_images.append({\n            \"id\": image_id,\n            \"file_name\": original_image_filename,\n            \"height\": image.shape[0],\n            \"width\": image.shape[1]\n        })\n\n        for bbox, category_id in zip(bboxes, category_ids):\n            augmented_annotations.append({\n                \"id\": annotation_id,\n                \"image_id\": image_id,\n                \"category_id\": category_id,\n                \"bbox\": bbox,\n                \"area\": bbox[2] * bbox[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            })\n            annotation_id += 1\n\n        # Save augmented images and annotations for each rotation\n        for angle in rotation_angles:\n            transforms = get_rotation_transforms(angle)\n            augmented = transforms(image=image, bboxes=bboxes, category_ids=category_ids)\n            augmented_image = augmented['image']\n            augmented_bboxes = augmented['bboxes']\n            augmented_category_ids = augmented['category_ids']\n\n            augmented_image_filename = f'augmented_image_{image_id}_{angle}.jpg'\n            augmented_image_path = os.path.join(output_images_folder, augmented_image_filename)\n            cv2.imwrite(augmented_image_path, augmented_image.permute(1, 2, 0).numpy())\n\n            augmented_images.append({\n                \"id\": image_id + angle // 90,  # Ensure unique IDs for augmented images\n                \"file_name\": augmented_image_filename,\n                \"height\": augmented_image.shape[1],\n                \"width\": augmented_image.shape[2]\n            })\n\n            for bbox, category_id in zip(augmented_bboxes, augmented_category_ids):\n                augmented_annotations.append({\n                    \"id\": annotation_id,\n                    \"image_id\": image_id + angle // 90,\n                    \"category_id\": category_id,\n                    \"bbox\": bbox,\n                    \"area\": bbox[2] * bbox[3],\n                    \"segmentation\": [],\n                    \"iscrowd\": 0\n                })\n                annotation_id += 1\n\n        image_id += 4  # Increment by 4 to account for the original and three augmented images\n\n    coco_data['images'] = augmented_images\n    coco_data['annotations'] = augmented_annotations\n    return coco_data","metadata":{"id":"9In9iZrCtlgf","execution":{"iopub.status.busy":"2024-07-16T11:15:54.121450Z","iopub.execute_input":"2024-07-16T11:15:54.121709Z","iopub.status.idle":"2024-07-16T11:15:54.139421Z","shell.execute_reply.started":"2024-07-16T11:15:54.121688Z","shell.execute_reply":"2024-07-16T11:15:54.138458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coco_json_path = f'/kaggle/working/merged_annotations1.json'\nimages_folder = f'/kaggle/working/merged_dataset1/'\noutput_images_folder = f'/kaggle/working/merged_dataset2'\n\n# Load annotations\ncoco_data = load_coco_annotations(coco_json_path)\n\n# Create output folder if it doesn't exist\nif not os.path.exists(output_images_folder):\n    os.makedirs(output_images_folder)\n\n# Augment images and annotations\ncoco=augment_images_and_annotations(coco_data, images_folder, output_images_folder)\nsave_coco_annotations(coco, f'aug_merged_annotations1.json')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T11:15:54.141096Z","iopub.execute_input":"2024-07-16T11:15:54.141391Z","iopub.status.idle":"2024-07-16T11:20:09.548176Z","shell.execute_reply.started":"2024-07-16T11:15:54.141369Z","shell.execute_reply":"2024-07-16T11:20:09.547320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir('/kaggle/working/merged_dataset2'))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T05:00:24.614123Z","iopub.execute_input":"2024-07-16T05:00:24.615342Z","iopub.status.idle":"2024-07-16T05:00:24.637012Z","shell.execute_reply.started":"2024-07-16T05:00:24.615290Z","shell.execute_reply":"2024-07-16T05:00:24.636092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"id":"vXf7hYrWtlgg","outputId":"a048f586-5159-4ea3-c33b-acc7d73b3579","execution":{"iopub.status.busy":"2024-07-16T07:21:56.260826Z","iopub.status.idle":"2024-07-16T07:21:56.261314Z","shell.execute_reply.started":"2024-07-16T07:21:56.261052Z","shell.execute_reply":"2024-07-16T07:21:56.261071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport os\nimport json\nfrom pathlib import Path\nimport yaml\nfrom random import shuffle\ndef convert_coco_to_yolo(coco_json_path, output_dir):\n    # Создаем директории для изображений\n    train_img_dir = Path(output_dir) / \"images\" / \"train\"\n    val_img_dir = Path(output_dir) / \"images\" / \"val\"\n    train_img_dir.mkdir(parents=True, exist_ok=True)\n    val_img_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Создаем директории для меток\n    train_lbl_dir = Path(output_dir) / \"labels\" / \"train\"\n    val_lbl_dir = Path(output_dir) / \"labels\" / \"val\"\n    train_lbl_dir.mkdir(parents=True, exist_ok=True)\n    val_lbl_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Загружаем аннотации COCO\n    with open(coco_json_path, 'r') as f:\n        coco_data = json.load(f)\n    \n    # Извлекаем категории\n    categories = {cat['id']: cat['name'] for cat in coco_data['categories']}\n    \n    # Разделяем данные на обучающую, валидационную и тестовую выборки\n    img_files = [img['file_name'] for img in coco_data['images']]\n    shuffle(img_files)\n    train_files = img_files[:18000]\n    val_files = img_files[18000:]\n    \n    # Создаем словарь для быстрого доступа к изображениям по имени файла и к аннотациям по image_id\n    img_dict = {img['file_name']: img for img in coco_data['images']}\n    ann_dict = {}\n    for ann in coco_data['annotations']:\n        image_id = ann['image_id']\n        if image_id not in ann_dict:\n            ann_dict[image_id] = []\n        ann_dict[image_id].append(ann)\n    \n    def save_files(files, img_target_dir, lbl_target_dir):\n        for img_file in tqdm(files):\n            img_path = img_target_dir / img_file\n            \n            # Копируем изображение в соответствующую директорию\n            original_img_path = Path(coco_json_path).parent / 'merged_dataset2' / img_file\n            img_path.write_bytes(original_img_path.read_bytes())\n            \n            # Создаем соответствующий файл аннотации\n            label_file = lbl_target_dir / f\"{img_path.stem}.txt\"\n            with open(label_file, 'w') as label_f:\n                img_info = img_dict[img_file]\n                image_id = img_info['id']\n                \n                if image_id in ann_dict:\n                    for ann in ann_dict[image_id]:\n                        category_id = ann['category_id'] - 1  # YOLO использует 0-индексацию категорий\n                        bbox = ann['bbox']\n                        # Конвертируем bbox в формат YOLO\n                        x_center = (bbox[0] + bbox[2] / 2) / img_info['width']\n                        y_center = (bbox[1] + bbox[3] / 2) / img_info['height']\n                        width = bbox[2] / img_info['width']\n                        height = bbox[3] / img_info['height']\n                        \n                        label_f.write(f\"{category_id} {x_center} {y_center} {width} {height}\\n\")\n    \n    # Сохраняем файлы\n    save_files(train_files, train_img_dir, train_lbl_dir)\n    save_files(val_files, val_img_dir, val_lbl_dir)\n    \n    # Создаем файл train.yaml\n    train_yaml_content = {\n        'path': '/kaggle/working/datasets',\n        'train': str(train_img_dir.resolve()),\n        'val': str(val_img_dir.resolve()),\n        'nc': len(categories),\n        'names': list(categories.values())\n    }\n    train_yaml_path = Path(output_dir) / \"train.yaml\"\n    with open(train_yaml_path, 'w') as f:\n        yaml.dump(train_yaml_content, f)\n\n# Пример использования\ncoco_json_path = '/kaggle/working/aug_merged_annotations1.json'\noutput_dir = '/kaggle/working/datasets'\nconvert_coco_to_yolo(coco_json_path, output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:30:47.967408Z","iopub.execute_input":"2024-07-16T07:30:47.967801Z","iopub.status.idle":"2024-07-16T07:30:56.402777Z","shell.execute_reply.started":"2024-07-16T07:30:47.967765Z","shell.execute_reply":"2024-07-16T07:30:56.401810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['WANDB_DISABLED'] = 'true'","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:31:16.433785Z","iopub.execute_input":"2024-07-16T07:31:16.434128Z","iopub.status.idle":"2024-07-16T07:31:16.441732Z","shell.execute_reply.started":"2024-07-16T07:31:16.434090Z","shell.execute_reply":"2024-07-16T07:31:16.440870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Предполагается, что 'last.pt' - это файл весов последней эпохи, который вы хотите загрузить.\n# Укажите корректный путь к файлу последней сохранённой модели.\nmodel = YOLO('/kaggle/input/electrocv/epoch8.pt')  # Замените на актуальный путь к вашему файлу с весами\n\n# Продолжить обучение модели с места последнего сохранения\nmodel.train(\n    data='/kaggle/working/datasets/train.yaml',\n    epochs=100,  # Общее количество эпох, до которого вы хотите обучить модель\n    resume=True,  # Указывает модели продолжить с последнего момента сохранения\n    save_period=1  # Период сохранения модели\n)","metadata":{"id":"-KDiizUcIiKW","outputId":"e2679b8f-6fb4-4af6-998b-5481bff536da","execution":{"iopub.status.busy":"2024-07-16T07:41:48.702962Z","iopub.execute_input":"2024-07-16T07:41:48.703381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}